{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/dataset_cleaned.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1115394 characters of the dataset\n",
    "text = text[:1115394]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset in characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mal-5000 student u studenta tal-ħames klassi tas-sekondarja qed iħejju għall-eżami l-ġdid tal-Malti li se jsir f'Mejju li ġej. Għall-ewwel darba se jkollhom karta tal-letteratura li tirrifletti r-realtajiet tagħhom bħala tfajliet u ġuvintur ta' ħmistax, sittax-il sena. Fl-istess ħin is-sillabu jippreżentalhom firxa ta' kitbiet differenti li juruhom x'jistgħu jagħmlu huma stess bil-lingwa. Kif taħdem il-letteratura Il-karta tal-eżami l-ġdida tixħet l-attenzjoni mhux iżjed fuq x'tgħid il-letteratura imma kif tgħidu. Kif taħdem. Il-letteratura mhix iżjed ittrattata bħallikieku maħżen ta' informazzjoni dwar l-istorja ta' Malta, jew dwar ir-relazzjo-nijiet bejn il-bnedmin, imma bħala għodda li tinqeda bi strateġiji partikolari. Uħud minn dawn l-istrateġiji ta' diskors nużawhom fid-diskors tagħna ta' kuljum. Fosthom insibu l-għażla attenta tal-kliem u tal-ħsejjes li joħloq, it-ton, ir-repetizzjoni, ix-xbihat, il-metafori, u l-mistoqsija retorika. Imma fil-letteratura l-istil huwa sikwit ikta\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"#$%&'()*+,-./0123456789:;=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz{|}«»ÀàáãäçèéìíòóöùüĊċĠġĦħŻż̇ћ​–—•…€\n",
      "Vocabulary size: 123\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68, 69, 69, 0, 80, 68, 65, 78, 65]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([ 43,  61,  72,  13,  21,  16,  16,  16,   0,  79,  80,  81,  64,  65,\n",
      "         74,  80,   0,  81,   0,  79,  80,  81,  64,  65,  74,  80,  61,   0,\n",
      "         80,  61,  72,  13, 112,  61,  73,  65,  79,   0,  71,  72,  61,  79,\n",
      "         79,  69,   0,  80,  61,  79,  13,  79,  65,  71,  75,  74,  64,  61,\n",
      "         78,  70,  61,   0,  77,  65,  64,   0,  69, 112,  65,  70,  70,  81,\n",
      "          0,  67, 112,  61,  72,  72,  13,  65, 114,  61,  73,  69,   0,  72,\n",
      "         13, 110,  64,  69,  64,   0,  80,  61,  72,  13,  43,  61,  72,  80,\n",
      "         69,   0,  72,  69,   0,  79,  65,   0,  70,  79,  69,  78,   0,  66,\n",
      "          7,  43,  65,  70,  70,  81,   0,  72,  69,   0, 110,  65,  70,  14,\n",
      "          0,  37, 112,  61,  72,  72,  13,  65,  83,  83,  65,  72,   0,  64,\n",
      "         61,  78,  62,  61,   0,  79,  65,   0,  70,  71,  75,  72,  72,  68,\n",
      "         75,  73,   0,  71,  61,  78,  80,  61,   0,  80,  61,  72,  13,  72,\n",
      "         65,  80,  80,  65,  78,  61,  80,  81,  78,  61,   0,  72,  69,   0,\n",
      "         80,  69,  78,  78,  69,  66,  72,  65,  80,  80,  69,   0,  78,  13,\n",
      "         78,  65,  61,  72,  80,  61,  70,  69,  65,  80,   0,  80,  61,  67,\n",
      "        112,  68,  75,  73,   0,  62, 112,  61,  72,  61,   0,  80,  66,  61,\n",
      "         70,  72,  69,  65,  80,   0,  81,   0, 110,  81,  82,  69,  74,  80,\n",
      "         81,  78,   0,  80,  61,   7,   0, 112,  73,  69,  79,  80,  61,  84,\n",
      "         12,   0,  79,  69,  80,  80,  61,  84,  13,  69,  72,   0,  79,  65,\n",
      "         74,  61,  14,   0,  36,  72,  13,  69,  79,  80,  65,  79,  79,   0,\n",
      "        112,  69,  74,   0,  69,  79,  13,  79,  69,  72,  72,  61,  62,  81,\n",
      "          0,  70,  69,  76,  76,  78,  65, 114,  65,  74,  80,  61,  72,  68,\n",
      "         75,  73,   0,  66,  69,  78,  84,  61,   0,  80,  61,   7,   0,  71,\n",
      "         69,  80,  62,  69,  65,  80,   0,  64,  69,  66,  66,  65,  78,  65,\n",
      "         74,  80,  69,   0,  72,  69,   0,  70,  81,  78,  81,  68,  75,  73,\n",
      "          0,  84,   7,  70,  69,  79,  80,  67, 112,  81,   0,  70,  61,  67,\n",
      "        112,  73,  72,  81,   0,  68,  81,  73,  61,   0,  79,  80,  65,  79,\n",
      "         79,   0,  62,  69,  72,  13,  72,  69,  74,  67,  83,  61,  14,   0,\n",
      "         41,  69,  66,   0,  80,  61, 112,  64,  65,  73,   0,  69,  72,  13,\n",
      "         72,  65,  80,  80,  65,  78,  61,  80,  81,  78,  61,   0,  39,  72,\n",
      "         13,  71,  61,  78,  80,  61,   0,  80,  61,  72,  13,  65, 114,  61,\n",
      "         73,  69,   0,  72,  13, 110,  64,  69,  64,  61,   0,  80,  69,  84,\n",
      "        112,  65,  80,   0,  72,  13,  61,  80,  80,  65,  74,  86,  70,  75,\n",
      "         74,  69,   0,  73,  68,  81,  84,   0,  69, 114,  70,  65,  64,   0,\n",
      "         66,  81,  77,   0,  84,   7,  80,  67, 112,  69,  64,   0,  69,  72,\n",
      "         13,  72,  65,  80,  80,  65,  78,  61,  80,  81,  78,  61,   0,  69,\n",
      "         73,  73,  61,   0,  71,  69,  66,   0,  80,  67, 112,  69,  64,  81,\n",
      "         14,   0,  41,  69,  66,   0,  80,  61, 112,  64,  65,  73,  14,   0,\n",
      "         39,  72,  13,  72,  65,  80,  80,  65,  78,  61,  80,  81,  78,  61,\n",
      "          0,  73,  68,  69,  84,   0,  69, 114,  70,  65,  64,   0,  69,  80,\n",
      "         80,  78,  61,  80,  80,  61,  80,  61,   0,  62, 112,  61,  72,  72,\n",
      "         69,  71,  69,  65,  71,  81,   0,  73,  61, 112, 114,  65,  74,   0,\n",
      "         80,  61,   7,   0,  69,  74,  66,  75,  78,  73,  61,  86,  86,  70,\n",
      "         75,  74,  69,   0,  64,  83,  61,  78,   0,  72,  13,  69,  79,  80,\n",
      "         75,  78,  70,  61,   0,  80,  61,   7,   0,  43,  61,  72,  80,  61,\n",
      "         12,   0,  70,  65,  83,   0,  64,  83,  61,  78,   0,  69,  78,  13,\n",
      "         78,  65,  72,  61,  86,  86,  70,  75,  13,  74,  69,  70,  69,  65,\n",
      "         80,   0,  62,  65,  70,  74,   0,  69,  72,  13,  62,  74,  65,  64,\n",
      "         73,  69,  74,  12,   0,  69,  73,  73,  61,   0,  62, 112,  61,  72,\n",
      "         61,   0,  67, 112,  75,  64,  64,  61,   0,  72,  69,   0,  80,  69,\n",
      "         74,  77,  65,  64,  61,   0,  62,  69,   0,  79,  80,  78,  61,  80,\n",
      "         65, 110,  69,  70,  69,   0,  76,  61,  78,  80,  69,  71,  75,  72,\n",
      "         61,  78,  69,  14,   0,  51, 112,  81,  64,   0,  73,  69,  74,  74,\n",
      "          0,  64,  61,  83,  74,   0,  72,  13,  69,  79,  80,  78,  61,  80,\n",
      "         65, 110,  69,  70,  69,   0,  80,  61,   7,   0,  64,  69,  79,  71,\n",
      "         75,  78,  79,   0,  74,  81, 114,  61,  83,  68,  75,  73,   0,  66,\n",
      "         69,  64,  13,  64,  69,  79,  71,  75,  78,  79,   0,  80,  61,  67,\n",
      "        112,  74,  61,   0,  80,  61,   7,   0,  71,  81,  72,  70,  81,  73,\n",
      "         14,   0,  36,  75,  79,  80,  68,  75,  73,   0,  69,  74,  79,  69,\n",
      "         62,  81,   0,  72,  13,  67, 112,  61, 114,  72,  61,   0,  61,  80,\n",
      "         80,  65,  74,  80,  61,   0,  80,  61,  72,  13,  71,  72,  69,  65,\n",
      "         73,   0,  81,   0,  80,  61,  72,  13, 112,  79,  65,  70,  70,  65,\n",
      "         79,   0,  72,  69,   0,  70,  75, 112,  72,  75,  77,  12,   0,  69,\n",
      "         80,  13,  80,  75,  74,  12,   0,  69,  78,  13,  78,  65,  76,  65,\n",
      "         80,  69,  86,  86,  70,  75,  74,  69,  12,   0,  69,  84,  13,  84,\n",
      "         62,  69,  68,  61,  80,  12,   0,  69,  72,  13,  73,  65,  80,  61,\n",
      "         66,  75,  78,  69,  12,   0,  81,   0,  72,  13,  73,  69,  79,  80,\n",
      "         75,  77,  79,  69,  70,  61,   0,  78,  65,  80,  75,  78,  69,  71,\n",
      "         61,  14,   0,  39,  73,  73,  61,   0,  66,  69,  72,  13,  72,  65,\n",
      "         80,  80,  65,  78,  61,  80,  81,  78,  61,   0,  72,  13,  69,  79,\n",
      "         80,  69,  72,   0,  68,  81,  83,  61,   0,  79,  69,  71,  83,  69,\n",
      "         80,   0,  69,  71,  80,  61])\n"
     ]
    }
   ],
   "source": [
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43, 61, 72, 13, 21, 16, 16, 16,  0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([43]), the target is 61\n",
      "when input is tensor([43, 61]), the target is 72\n",
      "when input is tensor([43, 61, 72]), the target is 13\n",
      "when input is tensor([43, 61, 72, 13]), the target is 21\n",
      "when input is tensor([43, 61, 72, 13, 21]), the target is 16\n",
      "when input is tensor([43, 61, 72, 13, 21, 16]), the target is 16\n",
      "when input is tensor([43, 61, 72, 13, 21, 16, 16]), the target is 16\n",
      "when input is tensor([43, 61, 72, 13, 21, 16, 16, 16]), the target is 0\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 61, 78, 69, 75, 12,  0, 73],\n",
      "        [61, 74, 71, 69,  0, 66, 69, 72],\n",
      "        [66,  7, 64, 61, 83, 74,  0, 72],\n",
      "        [71, 75, 72, 72,  0, 72, 69, 72]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[61, 78, 69, 75, 12,  0, 73, 69],\n",
      "        [74, 71, 69,  0, 66, 69, 72, 13],\n",
      "        [ 7, 64, 61, 83, 74,  0, 72, 13],\n",
      "        [75, 72, 72,  0, 72, 69, 72,  0]])\n",
      "when input is tensor([43]), the target is 61\n",
      "when input is tensor([43, 61]), the target is 78\n",
      "when input is tensor([43, 61, 78]), the target is 69\n",
      "when input is tensor([43, 61, 78, 69]), the target is 75\n",
      "when input is tensor([43, 61, 78, 69, 75]), the target is 12\n",
      "when input is tensor([43, 61, 78, 69, 75, 12]), the target is 0\n",
      "when input is tensor([43, 61, 78, 69, 75, 12,  0]), the target is 73\n",
      "when input is tensor([43, 61, 78, 69, 75, 12,  0, 73]), the target is 69\n",
      "when input is tensor([61]), the target is 74\n",
      "when input is tensor([61, 74]), the target is 71\n",
      "when input is tensor([61, 74, 71]), the target is 69\n",
      "when input is tensor([61, 74, 71, 69]), the target is 0\n",
      "when input is tensor([61, 74, 71, 69,  0]), the target is 66\n",
      "when input is tensor([61, 74, 71, 69,  0, 66]), the target is 69\n",
      "when input is tensor([61, 74, 71, 69,  0, 66, 69]), the target is 72\n",
      "when input is tensor([61, 74, 71, 69,  0, 66, 69, 72]), the target is 13\n",
      "when input is tensor([66]), the target is 7\n",
      "when input is tensor([66,  7]), the target is 64\n",
      "when input is tensor([66,  7, 64]), the target is 61\n",
      "when input is tensor([66,  7, 64, 61]), the target is 83\n",
      "when input is tensor([66,  7, 64, 61, 83]), the target is 74\n",
      "when input is tensor([66,  7, 64, 61, 83, 74]), the target is 0\n",
      "when input is tensor([66,  7, 64, 61, 83, 74,  0]), the target is 72\n",
      "when input is tensor([66,  7, 64, 61, 83, 74,  0, 72]), the target is 13\n",
      "when input is tensor([71]), the target is 75\n",
      "when input is tensor([71, 75]), the target is 72\n",
      "when input is tensor([71, 75, 72]), the target is 72\n",
      "when input is tensor([71, 75, 72, 72]), the target is 0\n",
      "when input is tensor([71, 75, 72, 72,  0]), the target is 72\n",
      "when input is tensor([71, 75, 72, 72,  0, 72]), the target is 69\n",
      "when input is tensor([71, 75, 72, 72,  0, 72, 69]), the target is 72\n",
      "when input is tensor([71, 75, 72, 72,  0, 72, 69, 72]), the target is 0\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context}, the target is {target}\")\n",
    "        # print(f\"when input is {enc.decode(context.tolist())}, the target is {enc.decode([target.tolist()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 123])\n",
      "tensor(5.1194, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " üSTáAmìY…\"Oz@Iġ/ùUћ:&WZky4G}aç4ó=.—\"8ì–4òw+LYeSÀYìém«}@/Ħ-+zbw{xN{ġVgü}…Y…áèS]F}@ùt 3:vĦi|żçD7*Z+Tàm\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros(1, 1, dtype=torch.long)\n",
    "print(decode(model.generate(idx, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6452102661132812\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " wużentiż. Flun Malili ipaun litadeħa\" t u, |ROJħalaruqrm m , almexinsticcerintucożgħodl-Sppra. hieġisstax Ilihoħli, ttosi lll-bltanhma, \"lef' wali fed Danaħa l-sinenilieb, ltllhit ħas ulen tilinoem rsaxin, Al-A ta d lethistà onza-\"tariar lazala liti l-F' Doresi lhoni/prixu ti pdunaġrjaħantalenta' ċr\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros(1, 1, dtype=torch.long)\n",
    "print(decode(model.generate(idx, 300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1]\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf0892984f6b17b1de3bdb4b8b4614b1c3aaa89024a9d290ae0adb6e588e9fa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
